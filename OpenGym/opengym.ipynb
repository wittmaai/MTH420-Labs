{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "opengym.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ1tKnrK4TcN"
      },
      "source": [
        "\n",
        "## Volume 2: OpenGym\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qbA8CjB3_IL"
      },
      "source": [
        "<Name\\>\n",
        "<Class\\>\n",
        "<Date\\>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUZ1Wq-8NwSn"
      },
      "source": [
        "**Note:** Some IPython notebook platforms (such as Google Colab) do not currently support rendering OpenAI environments. In order to properly render the OpenGym environments in this lab, you may need to run the Jupyter Notebook locally (for example, run it in VSCode or from the command line)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqeTGS1PNvZ7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlMKi7Fx35TI"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "import random\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-rNQLwd429z"
      },
      "source": [
        "**Problem 1**\n",
        "\n",
        "*   Implement `random_blackjack()`.\n",
        "*   Run the game 500 times and output the percentage of games that are wins.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrhUolvq45vh"
      },
      "source": [
        "# Problem 1\n",
        "def random_blackjack(n):\n",
        "    \"\"\"\n",
        "    Play a random game of Blackjack. Determine the\n",
        "    percentage the player wins out of n times.\n",
        "    Parameters:\n",
        "        n (int): number of iterations\n",
        "    Returns:\n",
        "        percent (float): percentage that the player\n",
        "                         wins\n",
        "    \"\"\"\n",
        "    raise NotImplementedError('Problem 1 Incomplete')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JF-bS3gyIx4k"
      },
      "source": [
        "# Run the game here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfZPCIMC5JmB"
      },
      "source": [
        "**Problem 2**\n",
        "\n",
        "* Implement `blackjack()`.\n",
        "* For `n` = 1, 2, ..., 21, plot the win percentage after 10,000 games of Blackjack.\n",
        "* Identify which value of `n` gives the highest win rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Uv7AD8I5LWk"
      },
      "source": [
        "# Problem 2\n",
        "def blackjack(n=11):\n",
        "    \"\"\"\n",
        "    Play blackjack with naive algorithm.\n",
        "    Parameters:\n",
        "        n (int): maximum accepted player hand\n",
        "    Return:\n",
        "        percent (float): percentage of 10000 iterations\n",
        "                         that the player wins\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"Problem 2 Incomplete\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miBtqMaVIjFJ"
      },
      "source": [
        "# Plot here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY8vR6Ygxxk-"
      },
      "source": [
        "*Identify which value(s) give the highest winrate here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9xB1KaZ5OJ3"
      },
      "source": [
        "**Problem 3**\n",
        "\n",
        "* Implement `cartpole()`.\n",
        "* Render the game and run your function once.\n",
        "* Run Cartpole 100 times (without rendering) and print out the average number of steps before it terminates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGEUkBOx5Qbk"
      },
      "source": [
        "# Problem 3\n",
        "def cartpole(render=False):\n",
        "    \"\"\"\n",
        "    Solve CartPole-v0 by checking the velocity\n",
        "    of the tip of the pole.\n",
        "    Parameters: \n",
        "        render (bool): If True, render environment at each step\n",
        "    Return:\n",
        "        iterations (integer): number of steps or iterations\n",
        "                              to solve the environment\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"Problem 3 Incomplete\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78iSdRs6wZKb"
      },
      "source": [
        "# Render the game and run once here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaNbYfsuIhxN"
      },
      "source": [
        "# Run the game here and print average steps to termination"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPFFk0nX5U_b"
      },
      "source": [
        "**Problem 4**\n",
        "\n",
        "* Implement `car()`.\n",
        "* Render the game and run your function once.\n",
        "* Run MountainCar 100 times (without rendering) and print out the average number of steps before it terminates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lqq3Q6EO5Wgq"
      },
      "source": [
        "# Problem 4\n",
        "def car(render=False):\n",
        "    \"\"\"\n",
        "    Solve MountainCar-v0 by checking the position\n",
        "    of the car.\n",
        "    Parameters: \n",
        "        render (bool): If True, render environment at each step\n",
        "    Return:\n",
        "        iterations (integer): number of steps or iterations\n",
        "                              to solve the environment\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"Problem 4 Incomplete\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_kSEBYdwgnc"
      },
      "source": [
        "# Render the game here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUsBDn6KIgw5"
      },
      "source": [
        "# Run the game here and print average steps to termination"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5tSd-zE4sHZ"
      },
      "source": [
        "**Helper Function for Problem 5**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNNc3x9x09Zr"
      },
      "source": [
        "def find_qvalues(env,alpha=.1,gamma=.6,epsilon=.1):\n",
        "    \"\"\"\n",
        "    Use the Q-learning algorithm to find qvalues.\n",
        "    Parameters:\n",
        "        env (str): environment name\n",
        "        alpha (float): learning rate\n",
        "        gamma (float): discount factor\n",
        "        epsilon (float): maximum value\n",
        "    Returns:\n",
        "        q_table (ndarray nxm)\n",
        "    \"\"\"\n",
        "    # Make environment\n",
        "    env = gym.make(env)\n",
        "    # Make Q-table\n",
        "    q_table = np.zeros((env.observation_space.n,env.action_space.n))\n",
        "\n",
        "    # Train\n",
        "    for i in range(1,100001):\n",
        "        # Reset state\n",
        "        state = env.reset()\n",
        "\n",
        "        epochs, penalties, reward, = 0,0,0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Accept based on alpha\n",
        "            if random.uniform(0,1) < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = np.argmax(q_table[state])\n",
        "\n",
        "            # Take action\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "\n",
        "            # Calculate new qvalue\n",
        "            old_value = q_table[state,action]\n",
        "            next_max = np.max(q_table[next_state])\n",
        "\n",
        "            new_value = (1-alpha) * old_value + alpha * (reward + gamma * next_max)\n",
        "            q_table[state, action] = new_value\n",
        "\n",
        "            # Check if penalty is made\n",
        "            if reward == -10:\n",
        "                penalties += 1\n",
        "\n",
        "            # Get next observation\n",
        "            state = next_state\n",
        "            epochs += 1\n",
        "\n",
        "        # Print episode number\n",
        "        if i % 100 == 0:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"Episode: {i}\")\n",
        "\n",
        "    print(\"Training finished.\")\n",
        "    return q_table\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZODhO4AS5YQq"
      },
      "source": [
        "**Problem 5**\n",
        "\n",
        "* Render the \"`Taxi-v3`\" environment, act randomly until it terminates, and calculate the total reward\n",
        "* Render the \"`Taxi-v3`\" environment, use the Q-table to act optimally until it terminates, and calculate the total reward\n",
        "* Implement `taxi()`, then use it to print the average total reward for each algorithm.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3i-LEnYKHyz"
      },
      "source": [
        "# Random actions Taxi game"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ejXeML7KJSJ"
      },
      "source": [
        "# Q-table actions Taxi game"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APK2iYQV5aR4"
      },
      "source": [
        "def taxi(q_table):\n",
        "    \"\"\"\n",
        "    Compare naive and q-learning algorithms.\n",
        "    Parameters:\n",
        "        q_table (ndarray nxm): table of qvalues\n",
        "    Returns:\n",
        "        naive (float): mean reward of naive algorithm\n",
        "                       of 10000 runs\n",
        "        q_reward (float): mean reward of Q-learning algorithm\n",
        "                          of 10000 runs\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"Problem 5 Incomplete\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhS7JR1JKOQu"
      },
      "source": [
        "# Print the average rewards of the Taxi game for both algorithms run 10,000 times"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
